{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5307b0b",
   "metadata": {},
   "source": [
    "Rupesh Bharambe (AI3107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a034ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math, re, requests\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b2838c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset (Alice in Wonderland)\n",
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "raw_text = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c895a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: remove Project Gutenberg license & clean\n",
    "start_idx = raw_text.find(\"THE SONNETS\")\n",
    "end_idx = raw_text.find(\"End of the Project Gutenberg\")\n",
    "text = raw_text[start_idx:end_idx]\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())  # remove punctuation\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65237b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocab = sorted(set(words))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e979fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 962800 | Vocab size: 29876\n"
     ]
    }
   ],
   "source": [
    "# Create sequences\n",
    "sequence_length = 10\n",
    "inputs, targets = [], []\n",
    "\n",
    "for i in range(len(words) - sequence_length):\n",
    "    seq = words[i:i+sequence_length]\n",
    "    target = words[i+sequence_length]\n",
    "    inputs.append([word2idx[w] for w in seq])\n",
    "    targets.append(word2idx[target])\n",
    "\n",
    "print(f\"Total sequences: {len(inputs)} | Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732db532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X)\n",
    "        self.Y = torch.tensor(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "dataset = WordDataset(inputs, targets)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2699b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class TransformerTextGen(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) * math.sqrt(x.size(-1))\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Target sequence attends to itself\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
    "        x = self.transformer_decoder(x.transpose(0, 1), x.transpose(0, 1), tgt_mask=tgt_mask)\n",
    "        return self.fc(x[-1])  # use only last token's output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef55b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.7674\n",
      "Epoch 2, Loss: 6.4719\n",
      "Epoch 3, Loss: 6.3805\n",
      "Epoch 4, Loss: 6.3282\n",
      "Epoch 5, Loss: 6.2945\n",
      "Epoch 6, Loss: 6.2631\n",
      "Epoch 7, Loss: 6.2417\n",
      "Epoch 8, Loss: 6.2095\n",
      "Epoch 9, Loss: 6.1867\n",
      "Epoch 10, Loss: 6.1690\n",
      "Epoch 11, Loss: 6.1504\n",
      "Epoch 12, Loss: 6.1333\n",
      "Epoch 13, Loss: 6.1195\n",
      "Epoch 14, Loss: 6.1135\n",
      "Epoch 15, Loss: 6.1106\n",
      "Epoch 16, Loss: 6.1089\n",
      "Epoch 17, Loss: 6.0824\n",
      "Epoch 18, Loss: 6.0788\n",
      "Epoch 19, Loss: 6.0827\n",
      "Epoch 20, Loss: 6.0582\n",
      "Epoch 21, Loss: 6.0561\n",
      "Epoch 22, Loss: 6.0325\n",
      "Epoch 23, Loss: 6.0471\n",
      "Epoch 24, Loss: 6.0425\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerTextGen(vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "for epoch in range(24):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for x_batch, y_batch in loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        out = model(x_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da3d5029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice was beginning to get very tired and and demetrius the cressids head absent makes the ones of war cannot offend henry this sweet queen distract the thing to fight and what enter parolles and rosencrantz richard must on your his body with the sixth of somerset blood and by a declining white well deserved a sail a man too i beseech you lear aside them to not the inclination of it enter sir pericles stand friar peace what good master word sir andrew let me see you know too i fear it utter we the madam and i care captain york me to bed your quits\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_text, num_words=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    words = seed_text.lower().split()\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        # Ensure input length = sequence_length (pad if short)\n",
    "        input_seq = words[-sequence_length:]\n",
    "        if len(input_seq) < sequence_length:\n",
    "            input_seq = ['<pad>'] * (sequence_length - len(input_seq)) + input_seq\n",
    "\n",
    "        # Convert words to indices\n",
    "        input_ids = [word2idx.get(w, word2idx.get('<unk>', 0)) for w in input_seq]\n",
    "        input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "            logits = logits / temperature  # Scale with temperature\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "            next_word_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        words.append(idx2word[next_word_id])\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "print(generate_text(model, \"alice was beginning to get very tired\", 100, temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a3785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch GPU)",
   "language": "python",
   "name": "pytorch-gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
